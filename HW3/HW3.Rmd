---
title: "Stats 315B: Homework 3"
author: "Joe Higgins, Austin Wang, Jessica Wetstone"
date: "Due 5/20/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(knitr)
library(nnet)
```

# Question 1
### Consider a multi-hidden layer neural network trained by sequential steepest-descent using the weight updating formula $w_t = w_{t-1} - \eta G(w_{t-1})$ Here $t$ labels the observations presented in sequence (time) and $G(w)$ is the gradient of the squared-error criterion evaluated at $w$. Derive a recursive "back-propagation" algorithm for updating all of the network weights at each step. With this algorithm the update for an input weight to a particular hidden node is computed using only the value of its corresponding input (that it weights), the value of the output of the hidden node to which it is input, and an "error signal" from each of the nodes in the next higher layer to which this node is connected. Thus, each node in the network can update its input weights using information provided only by the nodes to which it is connected.

# Question 2
### Consider a radial basis function network with spherical Gaussian basis of the form $B(x|\mu_m,\sigma_m) =  \left( -\frac{1}{2\sigma^{2}_{m}} \sum_{j=1}^{n} (x_j - \mu_{jm})^2 \right)$, with the function approximation given by $\hat{F}(x) = \sum_{m=1}^{M} a_m B(x|\mu_m,\sigma_m)$ and sum-of-squares error criterion. Derive expressions for the gradient $G(x)$ with respect to all (types of) parameters in the network.

# Question 3

# Question 4

# Question 5
### Describe $K$—fold cross-validation. What is it used for. What are the advan- tages/disadvantages of using more folds (increasing $K$). When does cross—validation estimate the performance of the actual predicting function being used.

# Question 6
### Suppose there are several outcome variables $\{y_1,y_2,...,y_M\}$ associated with a common set of predictor variables $x = \{x_1,x_2,...,x_n\}$. One could train separate single output neural networks for each outcome y_m or train a single network with multiple outputs, one for each y_m. What are the relative advantages/disadvantages of these two respective approaches. In what situations would one expect each to be better than the other.

# Question 7 
### Spam Email. The data sets spam_stats315B_train.csv, spam_stats315B_test.csv and documentation for this problem are the same as in Homework 2 and can be found in the class web page. You need first to standardize predictors and choose all the weights starting values at random in the interval [-0.5, 0.5].

```{r}
rm(list = ls())

#data labels
rflabs<-c("make", "address", "all", "3d", "our", "over", "remove",
  "internet","order", "mail", "receive", "will",
  "people", "report", "addresses","free", "business",
  "email", "you", "credit", "your", "font","000","money",
  "hp", "hpl", "george", "650", "lab", "labs",
  "telnet", "857", "data", "415", "85", "technology", "1999",
  "parts","pm", "direct", "cs", "meeting", "original", "project",
  "re","edu", "table", "conference", ";", "(", "[", "!", "$", "#",
  "CAPAVE", "CAPMAX", "CAPTOT","type")

#load the data
data_path <- paste(getwd(),'/data',sep='')
setwd(data_path)
train <- read.csv(file="spam_stats315B_train.csv", header=FALSE, sep=",")
test <- read.csv(file="spam_stats315B_test.csv", header=FALSE, sep=",")
colnames(train)<-rflabs
colnames(test)<-rflabs

#scale the train and test data
all_data <- rbind(train, test)

num_cols <- dim(all_data)[2]
all_data_X <- all_data[,c(1:(num_cols-1))]
data_scaled_X <- scale(all_data_X)

num_train <- nrow(train)
num_test <- nrow(test)

train_scaled_X <- data_scaled_X[c(1          :num_train),]
test_scaled_X  <- data_scaled_X[c((num_train+1):nrow(data_scaled_X)),]
train_y <- data.frame(train[,'type'])
test_y  <- data.frame(test[,'type'])

```

(a) Fit on the training set one hidden layer neural networks with 1, 2,..., 10 hidden units and different sets of starting values for the predictors (obtain in this way one model for each number of units). Which structural model performs best at classifying on the test set?

```{r}
wt_rang = 0.5
num_neurons <- seq(1:10)
models = list()

for(size in num_neurons){
  models <- c(models,
    list(nnet(
      train_scaled_X, train_y, size=num_neurons[size],
      linout = FALSE, entropy = FALSE, softmax = FALSE,
      censored = FALSE, skip = FALSE, rang = wt_rang, decay = 0,
      maxit = 5000, Hess = FALSE, trace = TRUE
    ))
  )
}

accuracy_rate <- function(y_hat, y, threshold){
  y_hat[y_hat >  threshold] <- 1
  y_hat[y_hat <= threshold] <- 0
  correct <- y_hat == y
  pct_correct = sum(correct)/length(correct)
  return(pct_correct)
}

threshold <- .50
y_hats <- lapply(models, function(x) { predict(x, test_scaled_X) })
accuracy <- lapply(y_hats, function(x) { accuracy_rate(x, test_y, threshold) })
best_performing <- which.max(accuracy)

print(best_performing)

```




