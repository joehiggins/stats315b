sigma2 <- 1
n_reps <- 10000
Beta_1_null <- 0
#Setup up null rejection
alpha <- 0.05
quantile <- 1-(alpha/2)
t_DoF <- n - p
null_not_rejected_region_min <- 0 - qt(quantile, t_DoF)
null_not_rejected_region_max <- 0 + qt(quantile, t_DoF)
null_not_rejected_count <- 0
null_not_rejected_properly_count <- 0
p_values <- rep(0,n_reps)
bayes_factors <- rep(0,n_reps)
posterior_probabilities <- rep(0,n_reps)
low_p_value_count <- 0
low_p_value_bayes_factors <- c()
low_p_value_posterior_probabilities <- c()
true_nulls_with_low_p_value_count <- 0
ps_vec <- rep(0,n_reps)
for(i in 1:n_reps){
#1. sample a nxp matrix X consisting of iid standard normal entries
X <- matrix(rnorm(n*p,mean=0,sd=sqrt(1)), n, p)
#2. let Beta_1 ~ N(0,1) with probability 1/2 and 0 with probability 1/2,
#       and all other elements of Beta ~ N(0,1)
Beta <- rnorm(p,mean=0,sd=sqrt(1))
if(runif(1,0,1) < 0.5)
Beta[1] <- 0
#3. Generate y from the gaussian linear model
#y <- mvrnorm(n = 1, mu = X%*%Beta, Sigma = covariance)
noise <- rnorm(n, mean = 0, sd = sqrt(sigma2))
y <- X%*%Beta + noise
Beta_hat <- solve((t(X)%*%X)) %*% t(X) %*% y
sigma_hat <- sqrt((1/(n-p)) * t(y - X %*% Beta_hat) %*% (y - X %*% Beta_hat))
t_sd <- sqrt(diag(solve(t(X) %*% X))) * c(sigma_hat)
t_stat <- (Beta_hat - Beta_1_null)/t_sd
fit <- lm(y~X-1)
summ <- summary(fit)
t_stat <- summ["coefficients"][[1]][21]
#4. test H_0: Beta_1 = 0 at level \alpha = 0.05 using the tests you specified in part a
if(null_not_rejected_region_min < t_stat && t_stat < null_not_rejected_region_max){
null_not_rejected_count <- null_not_rejected_count + 1
if(Beta[1] == 0){
null_not_rejected_properly_count <- null_not_rejected_properly_count + 1
}
}
#5. compute the p-value in the way you specified in part b
#p_value <- 2 * (1-pt(t_stat[1], t_DoF))
p_value <- summ["coefficients"][[1]][31]
p_values[i] <- p_value
#6. compute the Bayes factor and posterior probability that Beta_1 = 0 specified in part c
RSS <- sum(resid(fit)^2)
V = solve(t(X) %*% X)
X_0 <- X[,2:ncol(X)]
p_0 <- ncol(X_0)
fit_0 <- lm(y~X_0-1)
RSS_0 <- sum(resid(fit_0)^2)
V_0 = solve(t(X_0) %*% X_0)
py_M_0 <- (sqrt(det(V_0)) * gamma((n-p_0)/2))/((RSS_0 * pi)^((n-p_0)/2))
py_M   <- (sqrt(det(V))   * gamma((n-p)  /2))/((RSS   * pi)^((n-p  )/2))
p_M0_y = py_M_0/(py_M_0 + py_M)
bayes_factors[i] <- py_M_0/py_M
posterior_probabilities[i] <- p_M0_y
#f1. Consider the cases when p value between 0.01 and 0.05
#    What proportion of the time was the null hypothesis actually true in these cases?
#    What was the mean posterior probability of Beta_1 = 0 and the mean Bayes factor in these cases?
if(0.01 <= p_value && p_value <= 0.05){
low_p_value_count <- low_p_value_count + 1
low_p_value_bayes_factors <- union(low_p_value_bayes_factors, py_M_0/py_M)
low_p_value_posterior_probabilities <- union(low_p_value_posterior_probabilities, p_M0_y)
if(Beta[1] == 0){
true_nulls_with_low_p_value_count <- true_nulls_with_low_p_value_count + 1
}
}
}
null_not_rejected_count/null_not_rejected_properly_count
null_not_rejected_properly_count/null_not_rejected_count
rm(list = ls())
require('MASS')
require('mvtnorm')
set.seed(7)
#Problem, setup variables
n <- 50
p <- 10
sigma2 <- 1
n_reps <- 10000
Beta_1_null <- 0
#Setup up null rejection
alpha <- 0.05
quantile <- 1-(alpha/2)
t_DoF <- n - p
null_not_rejected_region_min <- 0 - qt(quantile, t_DoF)
null_not_rejected_region_max <- 0 + qt(quantile, t_DoF)
null_not_rejected_count <- 0
null_not_rejected_when_null_true_count <- 0
p_values <- rep(0,n_reps)
bayes_factors <- rep(0,n_reps)
posterior_probabilities <- rep(0,n_reps)
low_p_value_count <- 0
low_p_value_bayes_factors <- c()
low_p_value_posterior_probabilities <- c()
true_nulls_with_low_p_value_count <- 0
ps_vec <- rep(0,n_reps)
for(i in 1:n_reps){
#1. sample a nxp matrix X consisting of iid standard normal entries
X <- matrix(rnorm(n*p,mean=0,sd=sqrt(1)), n, p)
#2. let Beta_1 ~ N(0,1) with probability 1/2 and 0 with probability 1/2,
#       and all other elements of Beta ~ N(0,1)
Beta <- rnorm(p,mean=0,sd=sqrt(1))
if(runif(1,0,1) < 0.5)
Beta[1] <- 0
#3. Generate y from the gaussian linear model
#y <- mvrnorm(n = 1, mu = X%*%Beta, Sigma = covariance)
noise <- rnorm(n, mean = 0, sd = sqrt(sigma2))
y <- X%*%Beta + noise
Beta_hat <- solve((t(X)%*%X)) %*% t(X) %*% y
sigma_hat <- sqrt((1/(n-p)) * t(y - X %*% Beta_hat) %*% (y - X %*% Beta_hat))
t_sd <- sqrt(diag(solve(t(X) %*% X))) * c(sigma_hat)
t_stat <- (Beta_hat - Beta_1_null)/t_sd
fit <- lm(y~X-1)
summ <- summary(fit)
t_stat <- summ["coefficients"][[1]][21]
#4. test H_0: Beta_1 = 0 at level \alpha = 0.05 using the tests you specified in part a
if(null_not_rejected_region_min < t_stat && t_stat < null_not_rejected_region_max){
null_not_rejected_count <- null_not_rejected_count + 1
if(Beta[1] == 0){
null_not_rejected_when_null_true_count <- null_not_rejected_when_null_true_count + 1
}
}
#5. compute the p-value in the way you specified in part b
#p_value <- 2 * (1-pt(t_stat[1], t_DoF))
p_value <- summ["coefficients"][[1]][31]
p_values[i] <- p_value
#6. compute the Bayes factor and posterior probability that Beta_1 = 0 specified in part c
RSS <- sum(resid(fit)^2)
V = solve(t(X) %*% X)
X_0 <- X[,2:ncol(X)]
p_0 <- ncol(X_0)
fit_0 <- lm(y~X_0-1)
RSS_0 <- sum(resid(fit_0)^2)
V_0 = solve(t(X_0) %*% X_0)
py_M_0 <- (sqrt(det(V_0)) * gamma((n-p_0)/2))/((RSS_0 * pi)^((n-p_0)/2))
py_M   <- (sqrt(det(V))   * gamma((n-p)  /2))/((RSS   * pi)^((n-p  )/2))
p_M0_y = py_M_0/(py_M_0 + py_M)
bayes_factors[i] <- py_M_0/py_M
posterior_probabilities[i] <- p_M0_y
#f1. Consider the cases when p value between 0.01 and 0.05
#    What proportion of the time was the null hypothesis actually true in these cases?
#    What was the mean posterior probability of Beta_1 = 0 and the mean Bayes factor in these cases?
if(0.01 <= p_value && p_value <= 0.05){
low_p_value_count <- low_p_value_count + 1
low_p_value_bayes_factors <- union(low_p_value_bayes_factors, py_M_0/py_M)
low_p_value_posterior_probabilities <- union(low_p_value_posterior_probabilities, p_M0_y)
if(Beta[1] == 0){
true_nulls_with_low_p_value_count <- true_nulls_with_low_p_value_count + 1
}
}
}
null_not_rejected_when_null_true_count/null_not_rejected_count
(Beta_hat - Beta_1_null)/t_sd
(1-pt((Beta_hat - Beta_1_null)/t_sd, t_DoF))
#p_value <- 2 * (1-pt(((Beta_hat - Beta_1_null)/t_sd)[1], t_DoF))
2 * (1-pt(((Beta_hat - Beta_1_null)/t_sd)[1], t_DoF))
summ["coefficients"][[1]][31]
rm(list = ls())
require('MASS')
require('mvtnorm')
set.seed(7)
#Problem, setup variables
n <- 50
p <- 10
sigma2 <- 1
n_reps <- 10000
Beta_1_null <- 0
#Setup up null rejection
alpha <- 0.05
quantile <- 1-(alpha/2)
t_DoF <- n - p
null_not_rejected_region_min <- 0 - qt(quantile, t_DoF)
null_not_rejected_region_max <- 0 + qt(quantile, t_DoF)
null_not_rejected_count <- 0
null_not_rejected_when_null_true_count <- 0
p_values <- rep(0,n_reps)
bayes_factors <- rep(0,n_reps)
posterior_probabilities <- rep(0,n_reps)
low_p_value_count <- 0
low_p_value_bayes_factors <- c()
low_p_value_posterior_probabilities <- c()
true_nulls_with_low_p_value_count <- 0
ps_vec <- rep(0,n_reps)
for(i in 1:n_reps){
#1. sample a nxp matrix X consisting of iid standard normal entries
X <- matrix(rnorm(n*p,mean=0,sd=sqrt(1)), n, p)
#2. let Beta_1 ~ N(0,1) with probability 1/2 and 0 with probability 1/2,
#       and all other elements of Beta ~ N(0,1)
Beta <- rnorm(p,mean=0,sd=sqrt(1))
if(runif(1,0,1) < 2)
Beta[1] <- 0
#3. Generate y from the gaussian linear model
#y <- mvrnorm(n = 1, mu = X%*%Beta, Sigma = covariance)
noise <- rnorm(n, mean = 0, sd = sqrt(sigma2))
y <- X%*%Beta + noise
Beta_hat <- solve((t(X)%*%X)) %*% t(X) %*% y
sigma_hat <- sqrt((1/(n-p)) * t(y - X %*% Beta_hat) %*% (y - X %*% Beta_hat))
t_sd <- sqrt(diag(solve(t(X) %*% X))) * c(sigma_hat)
t_stat <- (Beta_hat - Beta_1_null)/t_sd
fit <- lm(y~X-1)
summ <- summary(fit)
t_stat <- summ["coefficients"][[1]][21]
#4. test H_0: Beta_1 = 0 at level \alpha = 0.05 using the tests you specified in part a
if(null_not_rejected_region_min < t_stat && t_stat < null_not_rejected_region_max){
null_not_rejected_count <- null_not_rejected_count + 1
if(Beta[1] == 0){
null_not_rejected_when_null_true_count <- null_not_rejected_when_null_true_count + 1
}
}
#5. compute the p-value in the way you specified in part b
#p_value <- 2 * (1-pt(((Beta_hat - Beta_1_null)/t_sd)[1], t_DoF))
p_value <- summ["coefficients"][[1]][31]
p_values[i] <- p_value
#6. compute the Bayes factor and posterior probability that Beta_1 = 0 specified in part c
RSS <- sum(resid(fit)^2)
V = solve(t(X) %*% X)
X_0 <- X[,2:ncol(X)]
p_0 <- ncol(X_0)
fit_0 <- lm(y~X_0-1)
RSS_0 <- sum(resid(fit_0)^2)
V_0 = solve(t(X_0) %*% X_0)
py_M_0 <- (sqrt(det(V_0)) * gamma((n-p_0)/2))/((RSS_0 * pi)^((n-p_0)/2))
py_M   <- (sqrt(det(V))   * gamma((n-p)  /2))/((RSS   * pi)^((n-p  )/2))
p_M0_y = py_M_0/(py_M_0 + py_M)
bayes_factors[i] <- py_M_0/py_M
posterior_probabilities[i] <- p_M0_y
#f1. Consider the cases when p value between 0.01 and 0.05
#    What proportion of the time was the null hypothesis actually true in these cases?
#    What was the mean posterior probability of Beta_1 = 0 and the mean Bayes factor in these cases?
if(0.01 <= p_value && p_value <= 0.05){
low_p_value_count <- low_p_value_count + 1
low_p_value_bayes_factors <- union(low_p_value_bayes_factors, py_M_0/py_M)
low_p_value_posterior_probabilities <- union(low_p_value_posterior_probabilities, p_M0_y)
if(Beta[1] == 0){
true_nulls_with_low_p_value_count <- true_nulls_with_low_p_value_count + 1
}
}
}
null_not_rejected_when_null_true_count/null_not_rejected_count
null_not_rejected_when_null_true_count
null_not_rejected_count
rm(list = ls())
require('MASS')
require('mvtnorm')
set.seed(7)
#Problem, setup variables
n <- 50
p <- 10
sigma2 <- 1
n_reps <- 10000
Beta_1_null <- 0
#Setup up null rejection
alpha <- 0.05
quantile <- 1-(alpha/2)
t_DoF <- n - p
null_not_rejected_region_min <- 0 - qt(quantile, t_DoF)
null_not_rejected_region_max <- 0 + qt(quantile, t_DoF)
null_not_rejected_count <- 0
null_not_rejected_when_null_true_count <- 0
p_values <- rep(0,n_reps)
bayes_factors <- rep(0,n_reps)
posterior_probabilities <- rep(0,n_reps)
low_p_value_count <- 0
low_p_value_bayes_factors <- c()
low_p_value_posterior_probabilities <- c()
true_nulls_with_low_p_value_count <- 0
ps_vec <- rep(0,n_reps)
for(i in 1:n_reps){
#1. sample a nxp matrix X consisting of iid standard normal entries
X <- matrix(rnorm(n*p,mean=0,sd=sqrt(1)), n, p)
#2. let Beta_1 ~ N(0,1) with probability 1/2 and 0 with probability 1/2,
#       and all other elements of Beta ~ N(0,1)
Beta <- rnorm(p,mean=0,sd=sqrt(1))
if(runif(1,0,1) < 0)
Beta[1] <- 0
#3. Generate y from the gaussian linear model
#y <- mvrnorm(n = 1, mu = X%*%Beta, Sigma = covariance)
noise <- rnorm(n, mean = 0, sd = sqrt(sigma2))
y <- X%*%Beta + noise
Beta_hat <- solve((t(X)%*%X)) %*% t(X) %*% y
sigma_hat <- sqrt((1/(n-p)) * t(y - X %*% Beta_hat) %*% (y - X %*% Beta_hat))
t_sd <- sqrt(diag(solve(t(X) %*% X))) * c(sigma_hat)
t_stat <- (Beta_hat - Beta_1_null)/t_sd
fit <- lm(y~X-1)
summ <- summary(fit)
t_stat <- summ["coefficients"][[1]][21]
#4. test H_0: Beta_1 = 0 at level \alpha = 0.05 using the tests you specified in part a
if(null_not_rejected_region_min < t_stat && t_stat < null_not_rejected_region_max){
null_not_rejected_count <- null_not_rejected_count + 1
if(Beta[1] == 0){
null_not_rejected_when_null_true_count <- null_not_rejected_when_null_true_count + 1
}
}
#5. compute the p-value in the way you specified in part b
#p_value <- 2 * (1-pt(((Beta_hat - Beta_1_null)/t_sd)[1], t_DoF))
p_value <- summ["coefficients"][[1]][31]
p_values[i] <- p_value
#6. compute the Bayes factor and posterior probability that Beta_1 = 0 specified in part c
RSS <- sum(resid(fit)^2)
V = solve(t(X) %*% X)
X_0 <- X[,2:ncol(X)]
p_0 <- ncol(X_0)
fit_0 <- lm(y~X_0-1)
RSS_0 <- sum(resid(fit_0)^2)
V_0 = solve(t(X_0) %*% X_0)
py_M_0 <- (sqrt(det(V_0)) * gamma((n-p_0)/2))/((RSS_0 * pi)^((n-p_0)/2))
py_M   <- (sqrt(det(V))   * gamma((n-p)  /2))/((RSS   * pi)^((n-p  )/2))
p_M0_y = py_M_0/(py_M_0 + py_M)
bayes_factors[i] <- py_M_0/py_M
posterior_probabilities[i] <- p_M0_y
#f1. Consider the cases when p value between 0.01 and 0.05
#    What proportion of the time was the null hypothesis actually true in these cases?
#    What was the mean posterior probability of Beta_1 = 0 and the mean Bayes factor in these cases?
if(0.01 <= p_value && p_value <= 0.05){
low_p_value_count <- low_p_value_count + 1
low_p_value_bayes_factors <- union(low_p_value_bayes_factors, py_M_0/py_M)
low_p_value_posterior_probabilities <- union(low_p_value_posterior_probabilities, p_M0_y)
if(Beta[1] == 0){
true_nulls_with_low_p_value_count <- true_nulls_with_low_p_value_count + 1
}
}
}
null_not_rejected_when_null_true_count/null_not_rejected_count
null_not_rejected_when_null_true_count
null_not_rejected_count
low_p_value_count
null_not_rejected_count
null_not_rejected_when_null_true_count/null_not_rejected_count
rm(list = ls())
set.seed(5)
Beta <- c(1)
p <- length(Beta)
true_sigma_squared <- 1
Beta_null <- 0
#beta_hat values to iterator over
n_values <- seq(3, 20)
n_rep <- 10000
alpha <- 0.05
quantile <- 1-(alpha/2)
Beta_hat_counts_outside_range <- matrix(0,p,length(n_values))
colnames(Beta_hat_counts_outside_range) <- n_values
t_stats_for_n5 <- matrix(0,n_rep,1)
for(i in 1:length(n_values)){
n <- n_values[i]
t_DoF <- n - p
null_not_reject_region_min <- 0 - qt(quantile, t_DoF)
null_not_reject_region_max <- 0 + qt(quantile, t_DoF)
for(j in 1:n_rep){
noise <- rnorm(n, mean = 0, sd = sqrt(true_sigma_squared))
X <- matrix(1, n, 1)
Y <- X %*% Beta + noise
Beta_hat <- solve((t(X)%*%X)) %*% t(X) %*% Y
sigma_hat <- sqrt((1/(n-p)) * t(Y - X %*% Beta_hat) %*% (Y - X %*% Beta_hat))
t_sd <- sqrt(diag(solve(t(X) %*% X))) * c(sigma_hat)
t_stat <- (Beta_hat - Beta_null)/t_sd
if(t_stat < null_not_reject_region_min || null_not_reject_region_max < t_stat){
Beta_hat_counts_outside_range[1,i] <- Beta_hat_counts_outside_range[1,i] + 1
}
if(n == 5){
t_stats_for_n5[j] <- t_stat
}
}
}
power <- Beta_hat_counts_outside_range/n_rep
rownames(power) <- "power"
plot(n_values, power)
hist(t_stats_for_n5, breaks = 100)
axis(side=1, at=seq(0,30,2))
warnings()
tree(house_data)
install.packages("tree")
library(tree)
tree(house_data)
library(rstudioapi)
library(rpart)
library(rpart.plot)
rm(list = ls())
# Question 2
#############
#Set working directory to data subdirectory
current_path <- getActiveDocumentContext()$path
current_directory <- dirname(current_path)
data_path <- paste(current_directory,'/data',sep='')
setwd(data_path)
#Read and type data
house_data <- read.csv('housetype_stats315B.csv')
factor_columns <- c(
'TypeHome',
'sex',
'MarStat',
'Occup',
'LiveBA',
'DualInc',
'HouseStat',
'Ethnic',
'Lang'
)
house_data[factor_columns] <- lapply(house_data[factor_columns], as.factor)
fit <- rpart(TypeHome ~ ., data = house_data, method = 'class')
rpart.plot(fit)
fit
#say something here
#misclassification
y_hat <- predict(fit, house_data)
y_hat_max_p <- apply(y_hat,1,which.max)
y <- house_data$TypeHome
correct <- y == y_hat_max_p
pct_correct <- sum(correct)/length(correct)
1-pct_correct
library(tree)
tree(house_data)
str(tree(house_data))
which(fit$frame$nsurrogate != 0) + 1
library(rstudioapi)
library(rpart)
library(rpart.plot)
rm(list = ls())
# Question 1
#############
#Set working directory to data subdirectory
current_path <- getActiveDocumentContext()$path
current_directory <- dirname(current_path)
data_path <- paste(current_directory,'/data',sep='')
setwd(data_path)
#Read and type data
age_data <- read.csv('age_stats315B.csv')
factor_columns <- c(
'Occup',
'TypeHome',
'sex',
'MarStat',
'DualInc',
'HouseStat',
'Ethnic',
'Lang'
)
age_data[factor_columns] <- lapply(age_data[factor_columns], as.factor)
fit <- rpart(age ~ ., data = age_data)
rpart.plot(fit)
fit
#First split: Marital status
#Strong indicators of you are not having ever been married.
#  2. Living together, not married
#  5. Single, never married
#For the unmarried...
#then the next thing is whether or not you own/rent your own place/live with parents
#there are two more splits, on education (younger: grade 11 or less, older: at least high school)
#                           and occupation (younger: not retired, retired)
# 1. Married
# 3. Divorced or separated
# 4. Widowed
#For the folks who have been married...
#then the next best split is occupation, retired vs. all other occupations
#Then the non-retired folks split on owning home vs. renting & living with family
#Then the folks who own a home, if they have less then half a person under 18 they are young
#a)
fit$frame$nsurrogate
which(fit$frame$nsurrogate != 0) + 1
colnames(age_data)[fit$frame$nsurrogate]
#yes there were surrogate splits used. Surrogates are variables used to classify data points
#that have missing values for a given feature The surrogate variables used were:
#b)
ncols <- dim(age_data)[2]
joe_data <- data.frame(matrix(ncol = ncols, nrow = 0))
#1,2,3,4,5,6,7,8,9,10,11,12,13,14
joe_data <- rbind(joe_data, c(3,1,3,1,5,6,9,3,1, 4, 0, 2, 8, 1))
colnames(joe_data) <- colnames(age_data)
joe_data[factor_columns] <- lapply(joe_data[factor_columns], factor)
predict(fit, joe_data)
#result: 2.8, on the upper end of 18 through 24. I'm 29. Gotcha tree!
library(rstudioapi)
library(rpart)
library(rpart.plot)
rm(list = ls())
# Question 2
#############
#Set working directory to data subdirectory
current_path <- getActiveDocumentContext()$path
current_directory <- dirname(current_path)
data_path <- paste(current_directory,'/data',sep='')
setwd(data_path)
#Read and type data
house_data <- read.csv('housetype_stats315B.csv')
factor_columns <- c(
'TypeHome',
'sex',
'MarStat',
'Occup',
'LiveBA',
'DualInc',
'HouseStat',
'Ethnic',
'Lang'
)
house_data[factor_columns] <- lapply(house_data[factor_columns], as.factor)
fit <- rpart(TypeHome ~ ., data = house_data, method = 'class')
rpart.plot(fit)
fit
#say something here
#misclassification
y_hat <- predict(fit, house_data)
y_hat_max_p <- apply(y_hat,1,which.max)
y <- house_data$TypeHome
correct <- y == y_hat_max_p
pct_correct <- sum(correct)/length(correct)
1-pct_correct
